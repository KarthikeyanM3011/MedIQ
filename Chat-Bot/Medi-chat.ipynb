{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b23b68cc-54f5-44bb-9c4b-e4e08d6cd2a3",
   "metadata": {},
   "source": [
    "#  Medicine Information Chatbot\r\n",
    "\r\n",
    "## Overview\r\n",
    "This Jupyter notebook documents the training process of a chatbot designed to analyze doctor prescriptions. The chatbot utilizes an Optical Character Recognition (OCR) model to extract medicine names from prescriptions. Once the medicine name is extracted, the chatbot provides information on the following aspects:\r",
    " \n",
    "- **Medicin Use**: Describes the purpose or use of the medicin \r\n",
    "- **Contraindic ions**: Specifies who should not take the medici  .\r\n",
    "- * osage**: Provides dosage information for the medic e.\r\n",
    "- **General Inf mation**: Offers general insights about the medicine to aid users in understanding and clarifying their doubts.\r\n",
    "\r\n",
    "## OCR Model\r\n",
    "The OCR model is employed to accurately extract medicine names from scanned doctor prescriptions. This ensures precise identification of medicines for further analysis.\r\n",
    "\r\n",
    "## Chatbot Functionality\r\n",
    "The chatbot is designed to respond to user queries regarding medicines identified from prescriptions. It provides comprehensive information including the medicine's purpose, contraindications, dosage guidelines, and additional details to facilitate users in understanding their medications better.\r\n",
    "\r\n",
    "## Data Sources\r\n",
    "The chatbot's knowledge base is constructed from reliable medical sources and databases to ensure accurate and up-to-date information.\r\n",
    "\r\n",
    "## Disclaimer\r\n",
    "This chatbot is intended for informational purposes only and should not substitute professional medical advice. Users are advised to consult healthcare professionals for personalized medical guidance and treatment recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1580c6e-406c-4945-84e0-1b34ae2fe94b",
   "metadata": {},
   "source": [
    "#Step 1: Setting Up the Environment üõ†Ô∏è\r\n",
    "First things first, let's get our environment ready! We'll install all the necessary packages, including the Hugging Face transformers library, datasets for easy data loading, wandb for experiment tracking, and a few others. üì¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b893ab9-1323-4368-9970-ab2f8e65612e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import site\n",
    "import os\n",
    "\n",
    "# Install the required packages\n",
    "!{sys.executable} -m pip install --upgrade  \"transformers>=4.38.*\"\n",
    "!{sys.executable} -m pip install --upgrade  \"datasets>=2.18.*\"\n",
    "!{sys.executable} -m pip install --upgrade \"wandb>=0.16.*\"\n",
    "!{sys.executable} -m pip install --upgrade \"trl>=0.7.11\"\n",
    "!{sys.executable} -m pip install --upgrade \"peft>=0.9.0\"\n",
    "!{sys.executable} -m pip install --upgrade \"accelerate>=0.28.*\"\n",
    "\n",
    "# Get the site-packages directory\n",
    "site_packages_dir = site.getsitepackages()[0]\n",
    "\n",
    "# add the site pkg directory where these pkgs are insalled to the top of sys.path\n",
    "if not os.access(site_packages_dir, os.W_OK):\n",
    "    user_site_packages_dir = site.getusersitepackages()\n",
    "    if user_site_packages_dir in sys.path:\n",
    "        sys.path.remove(user_site_packages_dir)\n",
    "    sys.path.insert(0, user_site_packages_dir)\n",
    "else:\n",
    "    if site_packages_dir in sys.path:\n",
    "        sys.path.remove(site_packages_dir)\n",
    "    sys.path.insert(0, site_packages_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5584e9-d4c2-4e1a-810e-86ef2c92566f",
   "metadata": {},
   "source": [
    "We'll now make sure to optimize our environment for the Intel GPU by setting the appropriate environment variables and configuring the number of cores and threads. This will ensure we get the best performance out of our hardware! ‚ö°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e33433-e7e1-48e2-b97f-d09ec63e02e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "import psutil\n",
    "num_physical_cores = psutil.cpu_count(logical=False)\n",
    "num_cores_per_socket = num_physical_cores // 2\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"0\"\n",
    "#HF_TOKEN = os.environ[\"HF_TOKEN\"]\n",
    "\n",
    "# St the LD_PRELOAD environment variable\n",
    "ld_preload = os.environ.get(\"LD_PRELOAD\", \"\")\n",
    "conda_prefix = os.environ.get(\"CONDA_PREFIX\", \"\")\n",
    "# Improve memory allocation performance, if tcmalloc is not available, please comment this line out\n",
    "os.environ[\"LD_PRELOAD\"] = f\"{ld_preload}:{conda_prefix}/lib/libtcmalloc.so\"\n",
    "# Reduce the overhead of submitting commands to the GPU\n",
    "os.environ[\"SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS\"] = \"1\"\n",
    "# reducing memory accesses by fusing SDP ops\n",
    "os.environ[\"ENABLE_SDP_FUSION\"] = \"1\"\n",
    "# set openMP threads to number of physical cores\n",
    "os.environ[\"OMP_NUM_THREADS\"] = str(num_physical_cores)\n",
    "# Set the thread affinity policy\n",
    "os.environ[\"OMP_PROC_BIND\"] = \"close\"\n",
    "# Set the places for thread pinning\n",
    "os.environ[\"OMP_PLACES\"] = \"cores\"\n",
    "\n",
    "print(f\"Number of physical cores: {num_physical_cores}\")\n",
    "print(f\"Number of cores per socket: {num_cores_per_socket}\")\n",
    "print(f\"OpenMP environment variables:\")\n",
    "print(f\"  - OMP_NUM_THREADS: {os.environ['OMP_NUM_THREADS']}\")\n",
    "print(f\"  - OMP_PROC_BIND: {os.environ['OMP_PROC_BIND']}\")\n",
    "print(f\"  - OMP_PLACES: {os.environ['OMP_PLACES']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4d0dfa-12d6-49e0-bf98-d223e8be5ca6",
   "metadata": {},
   "source": [
    "Step 2: Initializing the XPU and monitoring GPU memory in realtime üéÆ\r\n",
    "Next, we'll initialize the Intel Max 1550 GPU, which is referred to as an XPU. We'll use the intel_extension_for_pytorch library to seamlessly integrate XPU namespace with. ü§ù\r\n",
    "\r\n",
    "üëÄ GPU Memory Monitoring üëÄ\r\n",
    "To keep track of the Intel Max 1550 GPU (XPU) memory usage throughout this notebook, please refer to the cell below. It displays the current memory usage and updates every 5 seconds, providing you with real-time information about the GPU's memory consumption. üìä\r\n",
    "\r\n",
    "The memory monitoring cell displays the following information:\r\n",
    "\r\n",
    "XPU Device Name: The name of the Intel Max 1550 GPU being used.\r\n",
    "Reserved Memory: The amount of memory currently reserved by the GPU.\r\n",
    "Allocated Memory: The amount of memory currently allocated by the GPU.\r\n",
    "Max Reserved Memory: The maximum amount of memory that has been reserved by the GPU.\r\n",
    "Max Allocated Memory: The maximum amount of memory that has been allocated by the GPU.\r\n",
    "Keep an eye on this cell to monitor the GPU memory usage as you progress through the notebook. If you need to check the current memory usage at any point, simply scroll down to the memory monitoring cell for a quick reference. üëá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766e2678-42a1-4f55-84a8-620f763e4376",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import threading\n",
    "import torch\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import torch\n",
    "import intel_extension_for_pytorch as ipex\n",
    "\n",
    "if torch.xpu.is_available():\n",
    "    torch.xpu.empty_cache()\n",
    "    \n",
    "    def get_memory_usage():\n",
    "        memory_reserved = round(torch.xpu.memory_reserved() / 1024**3, 3)\n",
    "        memory_allocated = round(torch.xpu.memory_allocated() / 1024**3, 3)\n",
    "        max_memory_reserved = round(torch.xpu.max_memory_reserved() / 1024**3, 3)\n",
    "        max_memory_allocated = round(torch.xpu.max_memory_allocated() / 1024**3, 3)\n",
    "        return memory_reserved, memory_allocated, max_memory_reserved, max_memory_allocated\n",
    "   \n",
    "    def print_memory_usage():\n",
    "        device_name = torch.xpu.get_device_name()\n",
    "        print(f\"XPU Name: {device_name}\")\n",
    "        memory_reserved, memory_allocated, max_memory_reserved, max_memory_allocated = get_memory_usage()\n",
    "        memory_usage_text = f\"XPU Memory: Reserved={memory_reserved} GB, Allocated={memory_allocated} GB, Max Reserved={max_memory_reserved} GB, Max Allocated={max_memory_allocated} GB\"\n",
    "        print(f\"\\r{memory_usage_text}\", end=\"\", flush=True)\n",
    "    \n",
    "    async def display_memory_usage(output):\n",
    "        device_name = torch.xpu.get_device_name()\n",
    "        output.update(HTML(f\"<p>XPU Name: {device_name}</p>\"))\n",
    "        while True:\n",
    "            memory_reserved, memory_allocated, max_memory_reserved, max_memory_allocated = get_memory_usage()\n",
    "            memory_usage_text = f\"XPU ({device_name}) :: Memory: Reserved={memory_reserved} GB, Allocated={memory_allocated} GB, Max Reserved={max_memory_reserved} GB, Max Allocated={max_memory_allocated} GB\"\n",
    "            output.update(HTML(f\"<p>{memory_usage_text}</p>\"))\n",
    "            await asyncio.sleep(5)\n",
    "    \n",
    "    def start_memory_monitor(output):\n",
    "        loop = asyncio.new_event_loop()\n",
    "        asyncio.set_event_loop(loop)\n",
    "        loop.create_task(display_memory_usage(output))\n",
    "        thread = threading.Thread(target=loop.run_forever)\n",
    "        thread.start()    \n",
    "    output = display(display_id=True)\n",
    "    start_memory_monitor(output)\n",
    "else:\n",
    "    print(\"XPU device not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4999d3a-e019-426a-9643-9852938b7915",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install intel-extension-for-pytorch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a68706-c8b6-4681-9af8-f9854fc5ee7f",
   "metadata": {},
   "source": [
    "Step 3: Configuring the LoRA Settings üéõÔ∏è\r\n",
    "To finetune our Gemma model efficiently, we'll use the LoRA (Low-Rank Adaptation) technique.\r\n",
    "\r\n",
    "LoRA allows us to adapt the model to our specific task by training only a small set of additional parameters. This greatly reduces the training time and memory requirements! ‚è∞\r\n",
    "\r\n",
    "We'll define the LoRA configuration, specifying the rank (r) and the target modules we want to adapt. üéØ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19406b9f-42ca-436a-a19b-0e878332ce89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    # could use q, v and 0 projections as well and comment out the rest\n",
    "    target_modules=[\"q_proj\", \"o_proj\", \n",
    "                    \"v_proj\", \"k_proj\", \n",
    "                    \"gate_proj\", \"up_proj\",\n",
    "                    \"down_proj\"],\n",
    "    task_type=\"CAUSAL_LM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d2a740-550f-4985-8eb9-341222afe073",
   "metadata": {},
   "source": [
    "Step 4: Loading the Mixtral Model ü§ñ\r\n",
    "Now, let's load theMixtrala model using the Hugging Face AutoModelForCausalLM class. We'll also load the corresponding tokenizer to preprocess our input data. The model will be moved to the XPU for efficient training. üí™\r\n",
    "\r\n",
    "Note: Before running this notebook, please ensure you have read and agreed to tMixtralmma Terms of Use. You'll need to visit tMixtralmma model card on the Hugging Face Hub, accept the usage terms, and generate an access token with write permissions. This token will be required to load the model and push your finetuned version back to the Hub.\r\n",
    "\r\n",
    "To create an access token:\r\n",
    "\r\n",
    "Go to your Hugging Face account settings.\r\n",
    "Click on \"Access Tokens\" in the left sidebar.\r\n",
    "Click on the \"New token\" button.\r\n",
    "Give your token a name, select the desired permissions (make sure to include write access), and click \"Generate\".\r\n",
    "Copy the generated token and keep it secure. You'll use this token to authenticate when loading the model.\r\n",
    "Make sure to follow these steps to comply with the terms of use and ensure a smooth finetuning experience. If you have any questions or concerns, please refer to the official Gemma documentation or reach out to the Hugging Face community for assistance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fd6612-f72c-4e2f-942f-c6b7bb405776",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1137d197-46b7-4e52-9687-44c80b060ecf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51dcde9-8c26-4d08-a606-924abf12c0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "USE_CPU = False\n",
    "device = \"xpu:0\" if torch.xpu.is_available() else \"cpu\"\n",
    "if USE_CPU:\n",
    "    device = \"cpu\"\n",
    "print(f\"using device: {device}\")\n",
    "\n",
    "model_id = \"Intel/neural-chat-7b-v3-1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# Set padding side to the right to ensure proper attention masking during fine-tuning\n",
    "tokenizer.padding_side = \"right\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id).to(device)\n",
    "# Disable caching mechanism to reduce memory usage during fine-tuning\n",
    "model.config.use_cache = False\n",
    "# Configure the model's pre-training tensor parallelism degree to match the fine-tuning setup\n",
    "model.config.pretraining_tp = 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329425af-6ba0-474f-97aa-cec0ae4ee705",
   "metadata": {},
   "source": [
    "OCR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71214561-3fd7-46e5-a500-6074fa064417",
   "metadata": {},
   "source": [
    "Step 5: Testing the Model üß™\r\n",
    "Before we start finetuning, let's test the Gemma model on a sample input to see how it performs out-of-the-box. We'll generate some responses bsaed on a few questions in the test_inputs list below. üåø"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cd3eb9-77eb-4d20-aace-d122f4e8e21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from peft import LoraConfig, AutoPeftModelForCausalLM, prepare_model_for_kbit_training, get_peft_model\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GPTQConfig, TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "import os\n",
    "\n",
    "data = load_dataset(\"E:\\Anokha\\finalhyper.csv\", split=\"train\")\n",
    "data_df = data.to_pandas()\n",
    "\n",
    "data_df[\"text\"] = data_df[[\"medicine\", \"uses\", \"side  effect\"]].apply(lambda x: \"###Human: \" + x[\"medicine\"] + \" \" + x[\"uses\"] + \" ###Assistant: \"+ x[\"side effect\"], axis=1)\n",
    "data = Dataset.from_pandas(data_df)\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Intel/neural-chat-7b-v3-1\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "quantization_config_loading = GPTQConfig(bits=4, disable_exllama=True, tokenizer=tokenizer)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "                            \"Intel/neural-chat-7b-v3-1\",\n",
    "                            quantization_config=quantization_config_loading,\n",
    "                            device_map=\"auto\"\n",
    "                        )\n",
    "\n",
    "\n",
    "model.config.use_cache=False\n",
    "model.config.pretraining_tp=1\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=16, lora_alpha=16, lora_dropout=0.05, bias=\"none\", task_type=\"CAUSAL_LM\", target_modules=[\"q_proj\", \"v_proj\"]\n",
    ")\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "        output_dir=\"mistral-finetuned-alpaca\",\n",
    "        per_device_train_batch_size=8,\n",
    "        gradient_accumulation_steps=1,\n",
    "        optim=\"paged_adamw_32bit\",\n",
    "        learning_rate=2e-4,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_steps=100,\n",
    "        num_train_epochs=1,\n",
    "        max_steps=250,\n",
    "        fp16=True,\n",
    "        push_to_hub=True\n",
    ")\n",
    "\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=data,\n",
    "        peft_config=peft_config,\n",
    "        dataset_text_field=\"text\",\n",
    "        args=training_arguments,\n",
    "        tokenizer=tokenizer,\n",
    "        packing=False,\n",
    "        max_seq_length=512\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319da798-0de3-46b4-8ea3-43839e0e301f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import intel_extension_for_pytorch as ipex\n",
    "import argparse\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "import keras_ocr\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "\n",
    "\n",
    "def generate_response(model, query, prompt):\n",
    "    input_text = prompt.format(query=query)\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(device)\n",
    "    outputs = model.generate(input_ids, max_new_tokens=100, eos_token_id=tokenizer.eos_token_id)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "def test_model(model, query_list):\n",
    "    \"\"\"Test the model using the provided query list and prompt templates.\"\"\"\n",
    "    for query in query_list:\n",
    "        print(\"__\" * 25)\n",
    "        for prompt_template in [template1, template2, template3, template4]:\n",
    "            prompt = prompt_template.format(query=query)\n",
    "            generated_response = generate_response(model,  query, prompt_template)\n",
    "            print(f\"Prompt: {prompt}\")\n",
    "            print(f\"Generated Answer: {generated_response}\\n\")\n",
    "            print(\"__\" * 25)\n",
    "\n",
    "if _name_ == \"_main_\":\n",
    "\n",
    "\n",
    "    parser = argparse.ArgumentParser(\"Generation script (fp32/bf16 path)\", add_help=False)\n",
    "    parser.add_argument(\"--dtype\", type=str, choices=[\"float32\", \"bfloat16\"], default=\"float32\", help=\"choose the weight dtype and whether to enable auto mixed precision or not\")\n",
    "    parser.add_argument(\"--max-new-tokens\", default=32, type=int, help=\"output max new tokens\")\n",
    "    parser.add_argument(\"--prompt\", default=\"What are we having for dinner?\", type=str, help=\"input prompt\")\n",
    "    parser.add_argument(\"--greedy\", action=\"store_true\")\n",
    "    parser.add_argument(\"--batch-size\", default=1, type=int, help=\"batch size\")\n",
    "    args = parser.parse_args()\n",
    "    print(args)\n",
    "\n",
    "    amp_enabled = True if args.dtype != \"float32\" else False\n",
    "\n",
    "\n",
    "    model_id = MODEL_ID\n",
    "    config = AutoConfig.from_pretrained(model_id, torchscript=True, trust_remote_code=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float32, config=config, low_cpu_mem_usage=True, trust_remote_code=True)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "    model = model.eval()\n",
    "    model = model.to(memory_format=torch.channels_last)\n",
    "\n",
    "    model = ipex.llm.optimize(model, dtype=torch.float32, inplace=True, deployment_mode=True)\n",
    "\n",
    "    query_list = ['Paracetamol', 'ABCIXIMAB', 'ZOCLAR', 'GESTAKIND']\n",
    "    template1 = \"\"\"What is {query} used for?\"\"\"\n",
    "    template2 = \"\"\"Who should not take {query}?\"\"\"\n",
    "    template3 = \"\"\"How should I take this {query}?\"\"\"\n",
    "    template4 = \"\"\"General answer to a particular medicine {query}?\"\"\"\n",
    "\n",
    "    print(\"Testing the model before fine-tuning:\")\n",
    "    test_model(model, query_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea405a5e-25ea-424d-88af-38e976f343f6",
   "metadata": {},
   "source": [
    "# OutPut\n",
    "\n",
    "Prompt: What is Paracetamol used for?\r\n",
    "Generated Answer: What is Paracetamol used for?\r\n",
    "\r\n",
    "Paracetamol is used to relieve mild to moderate pain and to reduce fever. It is used to relieve pain from headaches, menstrual periods, toothaches, backaches, osteoarthritis, and other conditions.\r\n",
    "\r\n",
    "How does Paracetamol work?\r\n",
    "\r\n",
    "Paracetamol works by blocking the production of certain chemicals in the body that cause pain and\n",
    "\n",
    "Prompt: Who should not take Paracetamol?\r\n",
    "Generated Answer: Who should not take Paracetamol?\r\n",
    "\r\n",
    "Paracetamol is not suitable for some people. Tell your doctor or pharmacist if you:\r\n",
    "\r\n",
    "- have had an allergic reaction to paracetamol or any other medicines in the past\r\n",
    "- have liver problems\r\n",
    "- have a serious blood disorder called haemolytic anaemia\r\n",
    "- have a serious kidney disorder\r\n",
    "- have a serious heart condition\r\n",
    "- have a serious lung condition\r\n",
    "- have a serious stomac\n",
    "\n",
    "Prompt: How should I take this Paracetamol?\r\n",
    "Generated Answer: How should I take this Paracetamol?\r\n",
    "\r\n",
    "Paracetamol is available in many different forms, including tablets, capsules, soluble tablets, and liquid.\r\n",
    "\r\n",
    "The usual dose for adults and children aged 16 years and over is 1g (two 500mg tablets) taken up to four times a day, with a maximum dose of 4g (eight 500mg tablets) in 24 hours.\r\n",
    "\r\n",
    "For children \n",
    "\n",
    "Prompt: General answer to a particular medicine Paracetamol?\r\n",
    "Generated Answer: General answer to a particular medicine Paracetamol?\r\n",
    "\r\n",
    "Paracetamol is a medicine that is used to relieve pain and reduce fever. It is available in many different forms, including tablets, capsules, and liquids. Paracetamol is generally considered to be safe and effective when used as directed. However, it is important to follow the instructions on the label and not to take more than the recommended dose. Taking too much paracetamol can cause serious liver damag\n",
    "\n",
    "Prompt: What is ABCIXIMAB used for?\r\n",
    "Generated Answer: What is ABCIXIMAB used for?\r\n",
    "\r\n",
    "ABCIXIMAB is used to prevent blood clots from forming in people with certain heart or blood vessel conditions. It is also used to prevent blood clots from forming during or after heart bypass surgery.\r\n",
    "\r\n",
    "ABCIXIMAB is a monoclonal antibody that works by blocking the action of a protein in the body called platelet glycoprotein IIb/IIIa. This helps prevent platelets from sticking together to form \n",
    "\n",
    "Prompt: Who should not take ABCIXIMAB?\r\n",
    "Generated Answer: Who should not take ABCIXIMAB?\r\n",
    "\r\n",
    "Do not take ABCIXIMAB if you:\r\n",
    "\r\n",
    "- are allergic to abciximab or any of the ingredients in ABCIXIMAB. See the end of this leaflet for a complete list of ingredients in ABCIXIMAB.\r\n",
    "- have a history of bleeding problems\r\n",
    "- have had a stroke or a transient ischemic attack (TIA or ‚Äúmini-stroke‚Äù)\r\n",
    "- have had a recent serious injury or\n",
    "\n",
    "Prompt: How should I take this ABCIXIMAB?\r\n",
    "Generated Answer: How should I take this ABCIXIMAB?\r\n",
    "\r\n",
    "Take this medication exactly as prescribed by your doctor. Follow the directions on your prescription label.\r\n",
    "\r\n",
    "This medication is given as an infusion into a vein. It is usually given 18 to 24 hours before your scheduled procedure, and again just before the procedure.\r\n",
    "\r\n",
    "Your doctor will perform blood tests to make sure you do not have conditions that would prevent you from safely using ABC\n",
    "\n",
    "Prompt: What is ZOCLAR used for?\r\n",
    "Generated Answer: What is ZOCLAR used for?\r\n",
    "\r\n",
    "ZOCLAR is used to treat the symptoms of Parkinson's disease and other similar conditions, by increasing the levels of a natural substance called dopamine in the brain.\r\n",
    "\r\n",
    "How does ZOCLAR work?\r\n",
    "\r\n",
    "Parkinson's disease is caused by a lack of dopamine in the brain. Dopamine is a substance called a neurotransmitter, and it is involved in the control of voluntary movements. It works by stimulating the moto\n",
    "\n",
    "Prompt: Who should not take ZOCLAR?\r\n",
    "Generated Answer: Who should not take ZOCLAR?\r\n",
    "\r\n",
    "Do not take ZOCLAR if you:\r\n",
    "\r\n",
    "- are allergic to zolpidem tartrate or any of the ingredients in ZOCLAR. See the end of this leaflet for a complete list of ingredients in ZOCLAR.\r\n",
    "- have had an allergic reaction to drugs containing zolpidem, such as Ambien, Ambien CR, Edluar, Intermezzo, or Zolp\n",
    "\n",
    "  Prompt: How should I take this ZOCLAR?\r\n",
    "Generated Answer: How should I take this ZOCLAR?\r\n",
    "\r\n",
    "ZOCLAR is usually taken once a day, in the morning or in the evening, with or without food.\r\n",
    "\r\n",
    "Take ZOCLAR exactly as directed by your doctor. Do not take more or less of it or take it more often than prescribed by your doctor.\r\n",
    "\r\n",
    "Your doctor may start you on a low dose of ZOCLAR and gradually increase your dose.\r\n",
    "\r\n",
    "ZOCLAR controls high blood pressure but does not\n",
    "\n",
    "Prompt: General answer to a particular medicine ZOCLAR?\r\n",
    "Generated Answer: General answer to a particular medicine ZOCLAR?\r\n",
    "\r\n",
    "Zoclar is a brand name for the drug loratadine, which is an antihistamine used to treat symptoms of allergies such as sneezing, runny nose, and itchy, watery eyes. It works by blocking the action of histamine, a substance in the body that causes allergic symptoms. Zoclar is available in tablet and syrup form and is usually taken once a day. It is generally considered safe and effecti\n",
    "Prompt: What is GESTAKIND used for?\r\n",
    "Generated Answer: What is GESTAKIND used for?\r\n",
    "\r\n",
    "GESTAKIND is used to treat nausea and vomiting associated with pregnancy.\r\n",
    "\r\n",
    "How does GESTAKIND work?\r\n",
    "\r\n",
    "GESTAKIND contains dimenhydrinate, which is an antihistamine. Antihistamines work by blocking the action of histamine, a substance in the body that causes allergic symptoms. Dimenhydrinate also has anticholinergic properties, which means it can help to reduce the ac\n",
    "Prompt: How should I take this GESTAKIND?\r\n",
    "Generated Answer: How should I take this GESTAKIND?\r\n",
    "\r\n",
    "Take this medicine by mouth with a glass of water. Follow the directions on the prescription label. You can take it with or without food. If it upsets your stomach, take it with food. Take your medicine at regular intervals. Do not take your medicine more often than directed.\r\n",
    "\r\n",
    "Talk to your pediatrician regarding the use of this medicine in children. Special care may be needed.\r\n",
    "\r\n",
    "What should I tell my health care provider before I take this medicine?\r\n",
    "\r\n",
    "__________________________________________________\r\n",
    "Prompt: General answer to a particular medicine GESTAKIND?\r\n",
    "Generated Answer: General answer to a particular medicine GESTAKIND?\r\n",
    "\r\n",
    "The medicine GESTAKIND is a hormonal drug that is used to treat various gynecological diseases. It contains two active ingredients: gestodene and ethinylestradiol. These hormones work together to prevent ovulation, thicken cervical mucus, and thin the lining of the uterus.tivity\r\n",
    "ve\n",
    " cure it.imist.r nerveIXIMAB. surgeryclotsoutaged 12h condition fever."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11ad730-ffbb-45ca-8c53-cb2977f0d0f6",
   "metadata": {},
   "source": [
    "# Results\r\n",
    "Accurate Medicine Extraction: The OCR model successfully identifies and extracts medicine names from scanned prescriptions with high accuracy.\r\n",
    "Comprehensive Medicine Information: The chatbot provides detailed information about each medicine, including its purpose, contraindications, dosage guidelines, and general insights.\r\n",
    "User-Friendly Interface: The chatbot offers a user-friendly interface, allowing users to input prescriptions and receive instant medicine information.\r\n",
    "Conclusion\r\n",
    "The developed chatbot equipped with an OCR model facilitates efficient extraction and analysis of medicine names from doctor prescriptions. It serves as a valuable tool for users seeking comprehensive information about their medications, ultimately enhancing healthcare accessibility and awarene# ss.\r\n",
    "\r\n",
    "Future Directions\r\n",
    "Enhanced OCR Accuracy: Continuously refine the OCR model to improve accuracy and robustness in medicine name extraction.\r\n",
    "Expansion of Knowledge Base: Regularly update and expand the knowledge base to incorporate new medicines and updated information.\r\n",
    "Integration with Electronic Health Records: Explore integration with electronic health record systems to streamline medicine information retrieval for healthcare professionals.\r\n",
    "User Feedback Integration: Incorporate user feedback mechanisms to further enhance the chatbot's performance and usability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8acd5a-b6af-4c00-b63d-637ccba8ba7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
